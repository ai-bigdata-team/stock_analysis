# from google.cloud import bigquery
# import pandas as pd
#
# pd.set_option("display.max_columns", None)   # hi·ªán t·∫•t c·∫£ c·ªôt
# pd.set_option("display.width", None)         # kh√¥ng xu·ªëng d√≤ng g√£y
# pd.set_option("display.max_colwidth", None)  # hi·ªán full n·ªôi dung cell
#
# client = bigquery.Client(project="stockanalysis-480013")
#
# query = """
# SELECT *
# FROM `stockanalysis-480013.market_data.trades`
# """
#
# df = client.query(query).to_dataframe()
# print(df.head())
#
# # T·ªïng s·ªë d√≤ng
# total_rows = len(df)
#
# # S·ªë d√≤ng c√≥ c·ªôt open kh√°c NaN
# open_not_nan_rows = df["open"].notna().sum()
#
# print("T·ªïng s·ªë d√≤ng:", total_rows)
# print("S·ªë d√≤ng open kh√°c NaN:", open_not_nan_rows)
# distinct_rows = df.drop_duplicates().shape[0]
# print("T·ªïng s·ªë d√≤ng distinct:", distinct_rows)

import os
import sys
import subprocess

# ===== S·ª≠ d·ª•ng Java 11 =====
JAVA_11_HOME = r'C:\Program Files\Eclipse Adoptium\jdk-11.0.29.7-hotspot'

# ===== TH√äM HADOOP_HOME =====
os.environ['HADOOP_HOME'] = r'C:\hadoop'
os.environ['PATH'] = r'C:\hadoop\bin' + os.pathsep + os.environ.get('PATH', '')

java_exe = os.path.join(JAVA_11_HOME, 'bin', 'java.exe')
if not os.path.exists(java_exe):
    print(f"‚ùå ERROR: Kh√¥ng t√¨m th·∫•y Java t·∫°i: {JAVA_11_HOME}")
    sys.exit(1)

os.environ['JAVA_HOME'] = JAVA_11_HOME
os.environ['PATH'] = os.path.join(JAVA_11_HOME, 'bin') + os.pathsep + os.environ['PATH']
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

# ===== Google Cloud credentials =====
# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'F:\path\to\service-account-key.json'

print("üîç Checking Java version...")
result = subprocess.run([java_exe, '-version'], capture_output=True, text=True)
print(result.stderr)

# Ki·ªÉm tra winutils
winutils_path = r'C:\hadoop\bin\winutils.exe'
if not os.path.exists(winutils_path):
    print(f"\n‚ö†Ô∏è  WARNING: winutils.exe not found at {winutils_path}")
    print("   Download from: https://github.com/cdarlint/winutils/raw/master/hadoop-3.3.1/bin/winutils.exe")
    print("   And save to: C:\\hadoop\\bin\\winutils.exe")
    sys.exit(1)
else:
    print(f"‚úÖ Found winutils.exe at {winutils_path}")

print("\nüöÄ Starting PySpark with BigQuery connector...")
from pyspark.sql import SparkSession

try:
    spark = SparkSession.builder \
        .appName("BigQuery to Spark") \
        .master("local[*]") \
        .config("spark.driver.memory", "4g") \
        .config("spark.jars.packages", "com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.32.2") \
        .config("spark.hadoop.fs.defaultFS", "file:///") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("ERROR")

    print("\n‚úÖ Spark session created successfully!")
    print(f"   Spark version: {spark.version}")

    # ƒê·ªçc t·ª´ BigQuery
    print("\nüìä Reading from BigQuery...")

    df = spark.read \
        .format("bigquery") \
        .option("project", "stockanalysis-480013") \
        .option("dataset", "market_data") \
        .option("table", "trades") \
        .load()

    print("\nüìã Schema:")
    df.printSchema()

    print("\nüìä Sample data (first 20 rows):")
    df.show(20, truncate=False)

    print(f"\nüìà Total rows: {df.count()}")

    print("\n‚úÖ Successfully loaded data from BigQuery!")

    # spark.stop()

except Exception as e:
    print(f"\n‚ùå Error: {e}")
    import traceback

    traceback.print_exc()