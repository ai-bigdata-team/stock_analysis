# # from google.cloud import bigquery
# # import pandas as pd
# #
# # pd.set_option("display.max_columns", None)   # hi·ªán t·∫•t c·∫£ c·ªôt
# # pd.set_option("display.width", None)         # kh√¥ng xu·ªëng d√≤ng g√£y
# # pd.set_option("display.max_colwidth", None)  # hi·ªán full n·ªôi dung cell
# #
# # client = bigquery.Client(project="stockanalysis-480013")
# #
# # query = """
# # SELECT *
# # FROM `stockanalysis-480013.market_data.trades`
# # """
# #
# # df = client.query(query).to_dataframe()
# # print(df.head())
# #
# # # T·ªïng s·ªë d√≤ng
# # total_rows = len(df)
# #
# # # S·ªë d√≤ng c√≥ c·ªôt open kh√°c NaN
# # open_not_nan_rows = df["open"].notna().sum()
# #
# # print("T·ªïng s·ªë d√≤ng:", total_rows)
# # print("S·ªë d√≤ng open kh√°c NaN:", open_not_nan_rows)
# # distinct_rows = df.drop_duplicates().shape[0]
# # print("T·ªïng s·ªë d√≤ng distinct:", distinct_rows)
#
# import os
# import sys
# import subprocess
# from pyspark.sql import functions as F
# from pyspark.sql.window import Window
#
# # ===== S·ª≠ d·ª•ng Java 11 =====
# JAVA_11_HOME = r'C:\Program Files\Eclipse Adoptium\jdk-11.0.29.7-hotspot'
#
# # ===== TH√äM HADOOP_HOME =====
# os.environ['HADOOP_HOME'] = r'C:\hadoop'
# os.environ['PATH'] = r'C:\hadoop\bin' + os.pathsep + os.environ.get('PATH', '')
#
# java_exe = os.path.join(JAVA_11_HOME, 'bin', 'java.exe')
# if not os.path.exists(java_exe):
#     print(f"‚ùå ERROR: Kh√¥ng t√¨m th·∫•y Java t·∫°i: {JAVA_11_HOME}")
#     sys.exit(1)
#
# os.environ['JAVA_HOME'] = JAVA_11_HOME
# os.environ['PATH'] = os.path.join(JAVA_11_HOME, 'bin') + os.pathsep + os.environ['PATH']
# os.environ['PYSPARK_PYTHON'] = sys.executable
# os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
#
# print("üîç Checking Java version...")
# result = subprocess.run([java_exe, '-version'], capture_output=True, text=True)
# print(result.stderr)
#
# # Ki·ªÉm tra winutils
# winutils_path = r'C:\hadoop\bin\winutils.exe'
# if not os.path.exists(winutils_path):
#     print(f"\n‚ö†Ô∏è  WARNING: winutils.exe not found at {winutils_path}")
#     print("   Download from: https://github.com/cdarlint/winutils/raw/master/hadoop-3.3.1/bin/winutils.exe")
#     print("   And save to: C:\\hadoop\\bin\\winutils.exe")
#     sys.exit(1)
# else:
#     print(f"‚úÖ Found winutils.exe at {winutils_path}")
#
# print("\nüöÄ Starting PySpark with BigQuery connector...")
# from pyspark.sql import SparkSession
#
# try:
#     spark = SparkSession.builder \
#         .appName("BigQuery to Spark") \
#         .master("local[*]") \
#         .config("spark.driver.memory", "4g") \
#         .config("spark.hadoop.io.native.lib.available", "false") \
#         .config("spark.jars.packages", "com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.32.2") \
#         .config("spark.hadoop.fs.defaultFS", "file:///") \
#         .getOrCreate()
#
#     spark.sparkContext.setLogLevel("ERROR")
#
#     print("\n‚úÖ Spark session created successfully!")
#     print(f"   Spark version: {spark.version}")
#
#     # ƒê·ªçc t·ª´ BigQuery
#     print("\nüìä Reading from BigQuery...")
#
#     df = spark.read \
#         .format("bigquery") \
#         .option("project", "stockanalysis-480013") \
#         .option("dataset", "foreign_stock") \
#         .option("table", "tiingo") \
#         .load()
#
#     print("\nüìä Th√†nh c√¥ng")
#     df_spark = df.withColumnRenamed("symbol", "stock_code") \
#         .withColumnRenamed("date", "trade_timestamp")
#
#     # Chuy·ªÉn timestamp sang ƒë·ªãnh d·∫°ng chu·∫©n
#     df_spark = df_spark.withColumn("trade_timestamp", F.to_timestamp("trade_timestamp"))
#
#     # ===== Sinh c√°c ch·ªâ s·ªë t√†i ch√≠nh =====
#     df_spark = df_spark.withColumn(
#         "base_eps",
#         (F.col("close") % 4000 + 1000)
#     )
#
#     df_spark = df_spark.withColumn(
#         "EPS",
#         F.round(
#             F.col("base_eps") * (0.9 + F.rand() * 0.2),
#             0
#         )
#     )
#
#     df_spark = df_spark.withColumn(
#         "PE",
#         F.round(F.col("close") * 1000 / F.col("EPS"), 2)
#     )
#
#     df_spark = df_spark.withColumn("PB", F.round(1.0 + F.rand() * 4.0, 2))
#     df_spark = df_spark.withColumn("ROE", F.round(10.0 + F.rand() * 20.0, 2))
#     df_spark = df_spark.withColumn("ROA", F.round(5.0 + F.rand() * 10.0, 2))
#     df_spark = df_spark.withColumn("Beta", F.round(0.5 + F.rand() * 2.0, 2))
#     df_spark = df_spark.withColumn(
#         "MarketCap",
#         F.col("close") * F.col("volume") * 100
#     )
#
#     # X√≥a c·ªôt t·∫°m
#     df_spark = df_spark.drop("base_eps")
#
#     # Select c√°c c·ªôt c·∫ßn thi·∫øt
#     df_final = df_spark.select(
#         "trade_timestamp", "stock_code",
#         "open", "high", "low", "close", "volume",
#         "EPS", "PE", "PB", "ROE", "ROA", "Beta", "MarketCap"
#     )
#
#     print("\nüìã Schema:")
#     df_final.printSchema()
#
#     print("\nüìä Sample data (first 20 rows):")
#     df_final.show(20, truncate=False)
#
#     print(f"\nüìà Total rows: {df_final.count()}")
#
#     # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
#     output_dir = r"F:\TinHoc\BinningMini\BigData_BTL\stock_analysis\tests\loadData\stocks_ohlcv_new_batch"
#     os.makedirs(output_dir, exist_ok=True)
#
#     # L·∫•y danh s√°ch c√°c stock_code duy nh·∫•t
#     stock_codes = df_final.select("stock_code").distinct().collect()
#     stock_codes = [row.stock_code for row in stock_codes]
#
#     print(f"\nüì¶ Found {len(stock_codes)} unique stock codes")
#     print(f"üîÑ Processing each stock code...")
#
#     # L∆∞u t·ª´ng stock_code v√†o file ri√™ng
#     for i, stock_code in enumerate(stock_codes, 1):
#         print(f"  [{i}/{len(stock_codes)}] Processing {stock_code}...", end=" ")
#
#         # Filter data cho stock_code n√†y
#         df_stock = df_final.filter(F.col("stock_code") == stock_code)
#
#         # Chuy·ªÉn sang Pandas v√† l∆∞u
#         df_pandas = df_stock.toPandas()
#         df_pandas["trade_timestamp"] = df_pandas["trade_timestamp"].astype("datetime64[us]")
#
#         # ƒê∆∞·ªùng d·∫´n file output
#         output_path = os.path.join(output_dir, f"{stock_code}.parquet")
#
#         df_pandas.to_parquet(
#             output_path,
#             engine="pyarrow",
#             compression="snappy"
#         )
#
#         print(f"‚úÖ Saved {len(df_pandas)} rows")
#
#     print(f"\n‚úÖ Successfully saved {len(stock_codes)} files to {output_dir}")
#
#     spark.stop()
#
# except Exception as e:
#     print(f"\n‚ùå Error: {e}")
#     import traceback
#
#     traceback.print_exc()
#
#     traceback.print_exc()


import os
import pandas as pd

# ƒê∆∞·ªùng d·∫´n file input v√† th∆∞ m·ª•c output
input_file = r"F:\TinHoc\BinningMini\BigData_BTL\stock_analysis\tests\loadData\stocks_ohlcv_new.parquet"
output_dir = r"F:\TinHoc\BinningMini\BigData_BTL\stock_analysis\tests\loadData\stocks_ohlcv_new_batch"

# T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
os.makedirs(output_dir, exist_ok=True)

print("üìä Reading parquet file...")
# ƒê·ªçc file parquet
df = pd.read_parquet(input_file, engine="pyarrow")

print(f"‚úÖ Loaded {len(df)} rows")
print(f"üìã Columns: {list(df.columns)}")
print(f"\nüìä Sample data:")
print(df.head())

# L·∫•y danh s√°ch stock_code duy nh·∫•t
stock_codes = df['stock_code'].unique()
print(f"\nüì¶ Found {len(stock_codes)} unique stock codes")

# L∆∞u t·ª´ng stock_code v√†o file ri√™ng
print(f"üîÑ Processing each stock code...")
for i, stock_code in enumerate(stock_codes, 1):
    print(f"  [{i}/{len(stock_codes)}] Processing {stock_code}...", end=" ")

    # Filter data cho stock_code n√†y
    df_stock = df[df['stock_code'] == stock_code].copy()

    # ƒê∆∞·ªùng d·∫´n file output
    output_path = os.path.join(output_dir, f"{stock_code}.parquet")

    # L∆∞u file
    df_stock.to_parquet(
        output_path,
        engine="pyarrow",
        compression="snappy",
        index=False
    )

    print(f"‚úÖ Saved {len(df_stock)} rows")

print(f"\n‚úÖ Successfully saved {len(stock_codes)} files to {output_dir}")