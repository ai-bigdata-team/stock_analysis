networks:
  bigdata-network:
    driver: bridge

services:
  # ------------------ Spark ------------------
  spark-master:
    image: apache/spark:3.5.1
    container_name: spark-master
    environment:
      - SPARK_MASTER_HOST=spark-master
    command: ["/bin/bash", "-c", "/opt/spark/bin/spark-class org.apache.spark.deploy.master.Master"]
    networks:
      - bigdata-network
    ports:
      - "8081:8080"
      - "7077:7077"

  spark-worker-driver:
    image: apache/spark:3.5.1
    container_name: spark-worker-driver
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
      - HOME=/tmp
    networks:
      - bigdata-network
    volumes:
      - ./:/opt/spark/work-dir
      - ./.env:/opt/spark/.env
      - ./google-key.json:/opt/spark/google-key.json
      - ./requirements_spark.txt:/opt/spark/requirements_spark.txt
    command: ["/bin/bash", "-c", "pip install -r /opt/spark/requirements_spark.txt ; /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"]
    depends_on:
      - spark-master
    ports:
      - "4048:4040"

  spark-worker-2:
    image: apache/spark:3.5.1
    container_name: spark-worker-2
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
      - HOME=/tmp
    networks:
      - bigdata-network
    depends_on:
      - spark-master
    command: ["/bin/bash", "-c", "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"]

  spark-worker-3:
    image: apache/spark:3.5.1
    container_name: spark-worker-3
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
      - HOME=/tmp
    networks:
      - bigdata-network
    depends_on:
      - spark-master
    command: ["/bin/bash", "-c", "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"]

  # ------------------ Kafka ------------------
  kafka:
    image: apache/kafka:3.7.0
    container_name: kafka
    ports:
      - "19092:19092"
      - "19093:19093"
      - "19094:19094"
    networks:
      - bigdata-network
    environment:
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:19094"
      KAFKA_LISTENERS: "PLAINTEXT_HOST://0.0.0.0:19092,PLAINTEXT_INTERNAL://0.0.0.0:19093,CONTROLLER://0.0.0.0:19094"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT_HOST://localhost:19092,PLAINTEXT_INTERNAL://kafka:19093"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT_HOST:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT"
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_LOG_DIRS: "/var/lib/kafka/data"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_LOG_RETENTION_MS: 600000
      KAFKA_LOG_RETENTION_BYTES: 10737418240

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "18080:8080"
    networks:
      - bigdata-network
    depends_on:
      - kafka
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:19093

  # ------------------ Flink ------------------
  flink-jobmanager:
    image: my-flink:2.1.1
    container_name: flink-jobmanager
    hostname: flink-jobmanager
    networks:
      - bigdata-network
    ports:
      - "18081:8081"
    env_file:
      - .env
    volumes:
      - ./keys:/opt/flink/keys:ro
    command: jobmanager
    environment:
      KAFKA_BROKER: kafka:19093
      KAFKA_NODES: kafka:19093
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        jobmanager.memory.process.size: 1024m
        parallelism.default: 2
        restart-strategy: fixed-delay
        restart-strategy.fixed-delay.attempts: 3
        restart-strategy.fixed-delay.delay: 10s

  # ------------------ Flink TaskManager ------------------
  flink-taskmanager:
    image: my-flink:2.1.1
    container_name: flink-taskmanager
    hostname: flink-taskmanager
    env_file:
      - .env
    volumes:
      - ./keys:/opt/flink/keys:ro
    depends_on:
      - flink-jobmanager
    networks:
      - bigdata-network
    command: taskmanager
    environment:
      KAFKA_BROKER: kafka:19093
      KAFKA_NODES: kafka:19093
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
        taskmanager.memory.process.size: 2048m

  # ------------------ Airflow ------------------
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    networks:
      - bigdata-network
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data

  airflow-webserver:
    build: ./airflow
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW_UID=50000
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./.env:/opt/airflow/.env
    networks:
      - bigdata-network
    depends_on:
      - postgres
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    build: ./airflow
    container_name: airflow-scheduler
    command: scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW_UID=50000
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./.env:/opt/airflow/.env
    networks:
      - bigdata-network
    depends_on:
      - postgres

  airflow-init:
    build: ./airflow
    container_name: airflow-init
    # entrypoint: /bin/bash
    command:
      - bash
      - -c
      - |
        # Create the user
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin
        # Initialize the database
        airflow db migrate
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW_UID=50000
    networks:
      - bigdata-network
    depends_on:
      - postgres

volumes:
  postgres-db-volume:
