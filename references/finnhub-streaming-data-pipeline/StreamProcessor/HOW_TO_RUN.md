# ğŸš€ HÆ°á»›ng dáº«n cháº¡y StreamProcessor.scala

## ğŸ“‹ Tá»•ng quan

`StreamProcessor.scala` lÃ  Spark Structured Streaming application viáº¿t báº±ng Scala, Ä‘á»c dá»¯ liá»‡u tá»« Kafka vÃ  ghi vÃ o Cassandra.

## ğŸ¯ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka   â”‚ â”€â”€>  â”‚    Spark     â”‚ â”€â”€>  â”‚ Cassandra â”‚
â”‚ (Avro)   â”‚      â”‚ Streaming    â”‚      â”‚ (NoSQL)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ YÃªu cáº§u há»‡ thá»‘ng

### 1. Software cáº§n thiáº¿t:
- **Scala** 2.12.15
- **SBT** (Scala Build Tool) 1.5+
- **Apache Spark** 3.0.0
- **Kafka** 2.x+ Ä‘ang cháº¡y
- **Cassandra** 3.x+ Ä‘ang cháº¡y
- **Java** 8 hoáº·c 11

### 2. Kiá»ƒm tra version:
```bash
sbt --version         # SBT version
scala -version        # Scala version
java -version         # Java version
```

---

## ğŸ”§ CÃ¡ch 1: Cháº¡y trÃªn Local (KhÃ´ng dÃ¹ng Kubernetes)

### BÆ°á»›c 1: CÃ i Ä‘áº·t SBT (náº¿u chÆ°a cÃ³)

```bash
# Ubuntu/Debian
echo "deb https://repo.scala-sbt.org/scalasbt/debian all main" | sudo tee /etc/apt/sources.list.d/sbt.list
curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823" | sudo apt-key add
sudo apt-get update
sudo apt-get install sbt

# MacOS
brew install sbt
```

### BÆ°á»›c 2: Sá»­a file cáº¥u hÃ¬nh cho local

Chá»‰nh sá»­a `src/main/resources/application.conf`:

```conf
kafka {
    server = "localhost"          # Thay vÃ¬ kafka-service...
    port = "9092"
    topics {
        market = "finnhub_stock"  # Topic name cá»§a báº¡n
    }
}

cassandra {
    host = "localhost"            # Thay vÃ¬ cassandra
    keyspace = "market"
    username = "cassandra"
    password = "cassandra"
}

spark {
    master = "local[*]"           # Cháº¡y local thay vÃ¬ cluster
    appName {
        StreamProcessor = "Local Stream Processor"
    }
}
```

### BÆ°á»›c 3: Build project

```bash
cd /home/pavt1024/bigdata/stock_analysis/references/finnhub-streaming-data-pipeline/StreamProcessor

# Build JAR file
sbt clean assembly

# Káº¿t quáº£: táº¡o file target/scala-2.12/streamprocessor-assembly-1.0.jar
```

### BÆ°á»›c 4: Khá»Ÿi Ä‘á»™ng Cassandra (náº¿u chÆ°a cÃ³)

```bash
# Option 1: Docker
docker run --name cassandra -p 9042:9042 -d cassandra:3.11

# Option 2: Local installation
sudo service cassandra start

# Táº¡o keyspace vÃ  tables
cqlsh
```

Trong cqlsh, cháº¡y:
```cql
-- Táº¡o keyspace
CREATE KEYSPACE IF NOT EXISTS market 
WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

-- Táº¡o table trades
CREATE TABLE IF NOT EXISTS market.trades (
    uuid text PRIMARY KEY,
    trade_conditions list<text>,
    price double,
    symbol text,
    trade_timestamp timestamp,
    volume int,
    ingest_timestamp timestamp
);

-- Táº¡o table aggregates
CREATE TABLE IF NOT EXISTS market.running_averages_15_sec (
    uuid text PRIMARY KEY,
    symbol text,
    price_volume_multiply double,
    ingest_timestamp timestamp
);

-- Verify
DESCRIBE market.trades;
```

### BÆ°á»›c 5: Cháº¡y vá»›i spark-submit

```bash
spark-submit \
  --class StreamProcessor \
  --master local[*] \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,\
org.apache.spark:spark-avro_2.12:3.0.0,\
com.datastax.spark:spark-cassandra-connector_2.12:3.0.0 \
  --conf spark.cassandra.connection.host=localhost \
  --conf spark.cassandra.connection.port=9042 \
  --conf spark.cassandra.auth.username=cassandra \
  --conf spark.cassandra.auth.password=cassandra \
  --conf spark.sql.shuffle.partitions=4 \
  target/scala-2.12/streamprocessor-assembly-1.0.jar
```

---

## ğŸ³ CÃ¡ch 2: Cháº¡y vá»›i Docker

### BÆ°á»›c 1: Build Docker image

```bash
cd /home/pavt1024/bigdata/stock_analysis/references/finnhub-streaming-data-pipeline/StreamProcessor

# Build image
docker build -t streamprocessor:latest .
```

### BÆ°á»›c 2: Cháº¡y container

```bash
docker run \
  --network host \
  -e SPARK_MASTER_URL=spark://localhost:7077 \
  -e KAFKA_BOOTSTRAP_SERVERS=localhost:9092 \
  -e CASSANDRA_HOST=localhost \
  streamprocessor:latest
```

---

## â˜¸ï¸ CÃ¡ch 3: Cháº¡y trÃªn Kubernetes (Production)

### YÃªu cáº§u:
- Kubernetes cluster Ä‘ang cháº¡y
- Spark Operator Ä‘Ã£ cÃ i Ä‘áº·t
- Kafka vÃ  Cassandra Ä‘Ã£ deploy trÃªn K8s

### Deploy:

```bash
# Táº¡o SparkApplication resource
kubectl apply -f deployment.yaml

# Check logs
kubectl logs -f spark-streamprocessor-driver

# Check status
kubectl get sparkapplications
```

---

## ğŸ§ª Kiá»ƒm tra vÃ  Debug

### 1. Kiá»ƒm tra Kafka cÃ³ data khÃ´ng:

```bash
# List topics
kafka-topics.sh --list --bootstrap-server localhost:9092

# Consume messages
kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic finnhub_stock \
  --from-beginning \
  --max-messages 5
```

### 2. Kiá»ƒm tra Cassandra:

```bash
# VÃ o cqlsh
cqlsh localhost

# Query data
SELECT * FROM market.trades LIMIT 10;
SELECT * FROM market.running_averages_15_sec LIMIT 10;

# Count records
SELECT COUNT(*) FROM market.trades;
```

### 3. Xem Spark UI:

```bash
# Má»Ÿ browser
http://localhost:4040

# Tabs quan trá»ng:
# - Streaming: batch processing time, records processed
# - SQL: query execution plans
# - Executors: resource usage
```

### 4. Check logs:

```bash
# Náº¿u cháº¡y vá»›i spark-submit
tail -f /tmp/spark-events/*.log

# Náº¿u cháº¡y trong Docker
docker logs -f <container_id>

# Náº¿u cháº¡y trÃªn K8s
kubectl logs -f <pod_name>
```

---

## ğŸ” Troubleshooting

### Lá»—i: "Connection refused to Kafka"

```bash
# Kiá»ƒm tra Kafka Ä‘ang cháº¡y
netstat -an | grep 9092

# Test connection
telnet localhost 9092

# Sá»­a server address trong application.conf
kafka.server = "localhost"  # hoáº·c IP cá»§a Kafka broker
```

### Lá»—i: "NoHostAvailableException: Cassandra"

```bash
# Kiá»ƒm tra Cassandra Ä‘ang cháº¡y
cqlsh localhost 9042

# Verify connection trong spark-submit
--conf spark.cassandra.connection.host=localhost
--conf spark.cassandra.connection.port=9042
```

### Lá»—i: "ClassNotFoundException: AvroDataSourceReader"

```bash
# ThÃªm package vÃ o spark-submit
--packages org.apache.spark:spark-avro_2.12:3.0.0
```

### Lá»—i: "Assembly JAR not found"

```bash
# Rebuild project
cd StreamProcessor
sbt clean assembly

# Verify JAR tá»“n táº¡i
ls -lh target/scala-2.12/*.jar
```

---

## ğŸ“Š Monitoring trong Production

### 1. Metrics quan trá»ng:

```bash
# Processing rate
spark.streaming.receiver.recordsReceived

# Batch duration
spark.streaming.batchDuration

# Scheduling delay
spark.streaming.totalDelay
```

### 2. Alert conditions:

- Batch processing time > 10 seconds
- Scheduling delay tÄƒng dáº§n (backlog)
- Kafka consumer lag > 1000 records
- Cassandra write failures > 1%

### 3. Resource tuning:

```bash
spark-submit \
  --executor-memory 4G \
  --executor-cores 2 \
  --num-executors 3 \
  --conf spark.sql.shuffle.partitions=12 \
  ...
```

---

## ğŸ“š Files quan trá»ng

```
StreamProcessor/
â”œâ”€â”€ build.sbt                    # SBT build config
â”œâ”€â”€ src/main/scala/
â”‚   â””â”€â”€ StreamProcessor.scala    # Main application
â”œâ”€â”€ src/main/resources/
â”‚   â”œâ”€â”€ application.conf         # Config cho local
â”‚   â”œâ”€â”€ deployment.conf          # Config cho production
â”‚   â””â”€â”€ schemas/
â”‚       â””â”€â”€ trades.avsc          # Avro schema
â””â”€â”€ target/
    â””â”€â”€ scala-2.12/
        â””â”€â”€ streamprocessor-assembly-1.0.jar  # Compiled JAR
```

---

## ğŸ¯ Quick Start Command

Cháº¡y nhanh trÃªn local (táº¥t cáº£ services Ä‘Ã£ sáºµn sÃ ng):

```bash
# 1. Build
cd StreamProcessor && sbt assembly

# 2. Run
spark-submit \
  --class StreamProcessor \
  --master local[*] \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-avro_2.12:3.0.0,com.datastax.spark:spark-cassandra-connector_2.12:3.0.0 \
  target/scala-2.12/streamprocessor-assembly-1.0.jar

# 3. Monitor
open http://localhost:4040
```

---

## ğŸ’¡ Tips

1. **Development**: DÃ¹ng `local[*]` Ä‘á»ƒ test nhanh
2. **Staging**: Deploy trÃªn single-node Spark
3. **Production**: DÃ¹ng Spark cluster vá»›i HA
4. **Monitoring**: TÃ­ch há»£p Prometheus + Grafana
5. **Logging**: Centralized logging vá»›i ELK stack

---

Cáº§n há»— trá»£ thÃªm vá» pháº§n nÃ o khÃ´ng? ğŸš€
