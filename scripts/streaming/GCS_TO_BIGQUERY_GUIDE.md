# GCS to BigQuery Data Pipeline Guide

## ğŸ“‹ Tá»•ng quan

Sau khi Ä‘Ã£ stream dá»¯ liá»‡u tá»« Kafka vÃ o GCS, báº¡n cÃ³ 2 cÃ¡ch Ä‘á»ƒ Ä‘Æ°a dá»¯ liá»‡u vÃ o BigQuery:

### ğŸ”„ PhÆ°Æ¡ng Ã¡n 1: Batch Loading (Khuyáº¿n nghá»‹ cho historical data)
Äá»c dá»¯ liá»‡u tá»« GCS (Parquet) vÃ  load vÃ o BigQuery theo batch.

### âš¡ PhÆ°Æ¡ng Ã¡n 2: Direct Streaming (Khuyáº¿n nghá»‹ cho real-time)
Stream trá»±c tiáº¿p tá»« Kafka vÃ o BigQuery mÃ  khÃ´ng qua GCS.

---

## ğŸš€ BÆ°á»›c 1: Táº¡o BigQuery Dataset vÃ  Tables

```bash
# Cháº¡y SQL script Ä‘á»ƒ táº¡o schema
bq query --use_legacy_sql=false < scripts/streaming/bigquery_schema.sql

# Hoáº·c táº¡o manual qua Console
```

**LÆ°u Ã½:** Äáº£m báº£o BigQuery dataset Ä‘Ã£ Ä‘Æ°á»£c táº¡o trÆ°á»›c khi cháº¡y pipeline.

---

## ğŸ“¦ PhÆ°Æ¡ng Ã¡n 1: Batch Loading tá»« GCS

### Khi nÃ o dÃ¹ng?
- Load historical data Ä‘Ã£ cÃ³ sáºµn trong GCS
- KhÃ´ng cáº§n real-time, cháº¡y theo schedule (hourly, daily)
- Muá»‘n kiá»ƒm soÃ¡t tá»‘c Ä‘á»™ load Ä‘á»ƒ tiáº¿t kiá»‡m cost

### CÃ¡ch cháº¡y:

```bash
# Load táº¥t cáº£ dá»¯ liá»‡u (raw + aggregates)
python scripts/streaming/gcs_to_bigquery_batch.py \
    --project-id your-gcp-project-id \
    --dataset-id vnstock_data \
    --bucket-name stock_data_hehehe

# Chá»‰ load raw data
python scripts/streaming/gcs_to_bigquery_batch.py \
    --project-id your-gcp-project-id \
    --load-type raw

# Chá»‰ load aggregates
python scripts/streaming/gcs_to_bigquery_batch.py \
    --project-id your-gcp-project-id \
    --load-type aggregates
```

### Æ¯u Ä‘iá»ƒm:
âœ… Dá»… debug vÃ  retry  
âœ… CÃ³ thá»ƒ schedule vá»›i Airflow/Cron  
âœ… Kiá»ƒm soÃ¡t Ä‘Æ°á»£c cost tá»‘t hÆ¡n  
âœ… CÃ³ thá»ƒ dedup/validate trÆ°á»›c khi load  

### NhÆ°á»£c Ä‘iá»ƒm:
âŒ KhÃ´ng real-time  
âŒ Latency cao (phá»¥ thuá»™c schedule)  

---

## âš¡ PhÆ°Æ¡ng Ã¡n 2: Direct Streaming tá»« Kafka

### Khi nÃ o dÃ¹ng?
- Cáº§n data real-time trong BigQuery
- Dashboard/Analytics cáº§n refresh liÃªn tá»¥c
- KhÃ´ng quan tÃ¢m lÆ°u trá»¯ lÃ¢u dÃ i trÃªn GCS

### CÃ¡ch cháº¡y:

```bash
python scripts/streaming/vnstock_kafka_to_bigquery.py \
    --project-id your-gcp-project-id \
    --dataset-id vnstock_data \
    --bucket-name stock_data_hehehe \
    --kafka-servers localhost:9092 \
    --kafka-topic vnstock_stock \
    --window-duration "1 minutes"
```

### Æ¯u Ä‘iá»ƒm:
âœ… Real-time, latency tháº¥p (~30 seconds)  
âœ… KhÃ´ng cáº§n GCS storage cost  
âœ… Simple pipeline, Ã­t components  

### NhÆ°á»£c Ä‘iá»ƒm:
âŒ KhÃ³ debug khi cÃ³ lá»—i  
âŒ BigQuery write cost cao hÆ¡n  
âŒ Phá»¥ thuá»™c vÃ o Kafka uptime  

---

## ğŸ—ï¸ Kiáº¿n trÃºc Ä‘á» xuáº¥t (Hybrid)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka  â”‚ â”€â”€â”€> â”‚ Spark â”‚ â”€â”€â”€> â”‚   GCS   â”‚ â”€â”€â”€> â”‚ BigQuery â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                                 â–²
                     â”‚                                 â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           (Optional: Direct stream)
```

### Workflow:
1. **Real-time path**: Kafka â†’ Spark â†’ BigQuery (streaming)
2. **Backup path**: Kafka â†’ Spark â†’ GCS (cho archival)
3. **Batch re-load**: GCS â†’ BigQuery (khi cáº§n reprocess)

---

## ğŸ”§ Troubleshooting

### Lá»—i: "Table not found"
```bash
# Táº¡o tables trÆ°á»›c
bq query --use_legacy_sql=false < scripts/streaming/bigquery_schema.sql
```

### Lá»—i: "Permission denied"
```bash
# Kiá»ƒm tra service account permissions
gcloud projects get-iam-policy your-project-id \
    --flatten="bindings[].members" \
    --filter="bindings.members:serviceAccount:your-sa@project.iam.gserviceaccount.com"

# Cáº§n cÃ¡c roles:
# - BigQuery Data Editor
# - BigQuery Job User
# - Storage Object Admin
```

### Lá»—i: "Exceeded rate limits"
```python
# Giáº£m tá»‘c Ä‘á»™ ghi trong code
.trigger(processingTime="60 seconds")  # TÄƒng tá»« 30s lÃªn 60s
```

---

## ğŸ“Š Verify dá»¯ liá»‡u trong BigQuery

```bash
# Check record counts
bq query --use_legacy_sql=false '
SELECT 
  symbol,
  COUNT(*) as records,
  MIN(trade_timestamp) as first_record,
  MAX(trade_timestamp) as last_record
FROM `vnstock_data.vnstock_raw_ohlcv`
GROUP BY symbol
ORDER BY records DESC
'

# Check latest data
bq query --use_legacy_sql=false '
SELECT *
FROM `vnstock_data.vnstock_raw_ohlcv`
WHERE trade_date = CURRENT_DATE()
ORDER BY trade_timestamp DESC
LIMIT 10
'
```

---

## ğŸ’° Cost Optimization Tips

1. **Sá»­ dá»¥ng Partitioning**: Tables Ä‘Ã£ partition by date â†’ chá»‰ query data cáº§n thiáº¿t
2. **Clustering**: Cluster by `symbol` â†’ queries theo symbol sáº½ ráº¥t nhanh
3. **Batch size**: TÄƒng `processingTime` Ä‘á»ƒ giáº£m sá»‘ láº§n write
4. **Streaming inserts**: CÃ³ phÃ­ cao hÆ¡n batch, cÃ¢n nháº¯c trade-off
5. **GCS as backup**: LuÃ´n lÆ°u GCS Ä‘á»ƒ cÃ³ thá»ƒ re-load mÃ  khÃ´ng tá»‘n Kafka retention

---

## ğŸ¯ Khuyáº¿n nghá»‹ cuá»‘i cÃ¹ng

**Cho production:**
- Cháº¡y **cáº£ 2 pipelines** song song:
  - `vnstock_kafka_to_gcs.py` â†’ Backup to GCS
  - `vnstock_kafka_to_bigquery.py` â†’ Real-time to BigQuery
  
**Cho development/testing:**
- DÃ¹ng batch loading tá»« GCS
- Test queries trÆ°á»›c khi enable streaming

**Monitoring:**
- Setup alerts cho BigQuery quota limits
- Monitor Spark streaming lag
- Track BigQuery costs daily

---

## ğŸ“š References

- [Spark BigQuery Connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector)
- [BigQuery Partitioning](https://cloud.google.com/bigquery/docs/partitioned-tables)
- [Streaming Inserts Pricing](https://cloud.google.com/bigquery/pricing#streaming_pricing)
