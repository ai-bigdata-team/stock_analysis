"""
PySpark Structured Streaming job: Kafka â†’ Transform â†’ Cassandra/Parquet
ÄÃ¢y lÃ  phiÃªn báº£n Python tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i StreamProcessor.scala
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, from_json, explode, current_timestamp, 
    avg, window, udf
)
from pyspark.sql.types import StringType
import uuid

# Import module xá»­ lÃ½ cá»§a chÃºng ta
from finnhub_processing import schema, process_stocktrade_data_realtime


def create_spark_session(app_name="Finnhub-Kafka-Streaming"):
    """Táº¡o Spark Session vá»›i cáº¥u hÃ¬nh cáº§n thiáº¿t"""
    return SparkSession.builder \
        .appName(app_name) \
        .config("spark.sql.shuffle.partitions", "4") \
        .config("spark.cassandra.connection.host", "localhost") \
        .config("spark.cassandra.auth.username", "cassandra") \
        .config("spark.cassandra.auth.password", "cassandra") \
        .getOrCreate()


def read_from_kafka(spark, kafka_servers="localhost:9092", topic="finnhub_stock"):
    """Äá»c streaming data tá»« Kafka"""
    return spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", kafka_servers) \
        .option("subscribe", topic) \
        .option("startingOffsets", "latest") \
        .option("maxOffsetsPerTrigger", 10000) \
        .load()


def parse_kafka_messages(df, schema):
    """
    Parse JSON messages tá»« Kafka value column.
    Giáº£ sá»­ message format: {"data": [{"p":123, "s":"AAPL", ...}], "type":"trade"}
    """
    # Parse JSON tá»« value column
    parsed_df = df.select(
        from_json(col("value").cast("string"), schema).alias("parsed_data")
    )
    
    # Explode array "data" Ä‘á»ƒ cÃ³ tá»«ng trade riÃªng láº»
    exploded_df = parsed_df.select(
        explode(col("parsed_data.data")).alias("trade")
    )
    
    # Extract cÃ¡c trÆ°á»ng tá»« trade
    trades_df = exploded_df.select(
        col("trade.c").alias("c"),
        col("trade.p").alias("p"),
        col("trade.s").alias("s"),
        col("trade.t").alias("t"),
        col("trade.v").alias("v")
    )
    
    return trades_df


def add_uuid_column(df):
    """ThÃªm UUID column cho Cassandra primary key"""
    uuid_udf = udf(lambda: str(uuid.uuid1()), StringType())
    return df.withColumn("uuid", uuid_udf())


def write_to_cassandra(df, keyspace="stock_data", table="trades"):
    """Ghi streaming data vÃ o Cassandra"""
    return df \
        .writeStream \
        .foreachBatch(lambda batch_df, batch_id: 
            batch_df.write
                .format("org.apache.spark.sql.cassandra")
                .options(table=table, keyspace=keyspace)
                .mode("append")
                .save()
        ) \
        .outputMode("update") \
        .start()


def write_to_parquet(df, output_path, checkpoint_path):
    """Ghi streaming data vÃ o Parquet files (alternative to Cassandra)"""
    return df \
        .writeStream \
        .format("parquet") \
        .option("path", output_path) \
        .option("checkpointLocation", checkpoint_path) \
        .outputMode("append") \
        .trigger(processingTime="10 seconds") \
        .start()


def create_aggregates(df):
    """
    Táº¡o aggregates: tÃ­nh giÃ¡ trung bÃ¬nh theo symbol má»—i 15 giÃ¢y
    TÆ°Æ¡ng tá»± nhÆ° summaryDF trong Scala version
    """
    return df \
        .withColumn("price_volume_multiply", col("price") * col("volume")) \
        .withWatermark("event_time", "15 seconds") \
        .groupBy("symbol", window("event_time", "15 seconds")) \
        .agg(avg("price_volume_multiply").alias("avg_price_volume"))


def main():
    """Main function - orchestrate toÃ n bá»™ pipeline"""
    
    # 1. Táº¡o Spark session
    print("ğŸš€ Starting Spark Streaming Job...")
    spark = create_spark_session()
    
    # 2. Äá»c tá»« Kafka
    print("ğŸ“¥ Reading from Kafka...")
    kafka_df = read_from_kafka(
        spark, 
        kafka_servers="localhost:9092",
        topic="finnhub_stock"
    )
    
    # 3. Parse JSON messages
    print("ğŸ”§ Parsing Kafka messages...")
    # Cáº§n Ä‘á»‹nh nghÄ©a schema cho message wrapper
    from pyspark.sql.types import StructType, StructField, ArrayType
    
    message_schema = StructType([
        StructField("data", ArrayType(schema), True),
        StructField("type", StringType(), True)
    ])
    
    trades_df = parse_kafka_messages(kafka_df, message_schema)
    
    # 4. Transform data vá»›i finnhub_processing module
    print("âš™ï¸ Transforming data...")
    transformed_df = process_stocktrade_data_realtime(trades_df)
    
    # ThÃªm ingest timestamp vÃ  UUID
    final_df = transformed_df \
        .withColumn("ingest_timestamp", current_timestamp()) \
        .withColumn("uuid", udf(lambda: str(uuid.uuid1()), StringType())())
    
    # 5. Táº¡o aggregates
    print("ğŸ“Š Creating aggregates...")
    aggregates_df = create_aggregates(final_df)
    
    # 6A. Ghi vÃ o Parquet (dá»… test hÆ¡n Cassandra)
    print("ğŸ’¾ Writing to Parquet...")
    query1 = write_to_parquet(
        final_df,
        output_path="./output/trades",
        checkpoint_path="./checkpoints/trades"
    )
    
    query2 = write_to_parquet(
        aggregates_df,
        output_path="./output/aggregates",
        checkpoint_path="./checkpoints/aggregates"
    )
    
    # 6B. Hoáº·c ghi vÃ o Cassandra (uncomment náº¿u cÃ³ Cassandra)
    # query1 = write_to_cassandra(final_df, table="trades")
    # query2 = write_to_cassandra(aggregates_df, table="aggregates")
    
    # 7. Äá»£i job cháº¡y
    print("âœ… Streaming job is running...")
    print("ğŸ“ Check output at: ./output/trades and ./output/aggregates")
    print("ğŸ›‘ Press Ctrl+C to stop")
    
    spark.streams.awaitAnyTermination()


if __name__ == "__main__":
    main()
